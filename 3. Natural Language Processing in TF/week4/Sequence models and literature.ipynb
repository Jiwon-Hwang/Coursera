{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sequence models and literature.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMEqfVJDOXrqVXwuvtaa1y6"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZEMVFV9Ry-rc"},"source":["## Introduction\r\n","* 이전 강의 : classification of text\r\n","* 이번 강의 : generate new text (--> prediction problem)\r\n","  \r\n","ex)\r\n","> * train_Xs : Twinkle, Twinkle, Little\r\n","> * train_Ys : Star  \r\n","이라면,  \r\n","> * test_Xs : Twinkle, Twinkle, Little\r\n","> * 'prediction_Ys' : ***'Star'***"]},{"cell_type":"markdown","metadata":{"id":"LUmwqoLb0U7Q"},"source":["## Looking into the code"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DauitOeix8SW","executionInfo":{"status":"ok","timestamp":1612925479305,"user_tz":-540,"elapsed":2725,"user":{"displayName":"황지원","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3QkHCcEEvrl0G2GezbnmqYv8hfEf2EXH8W_8vNw=s64","userId":"04698757380061261060"}},"outputId":"7ea61a22-846b-4420-ac75-33220d48f403"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\r\n","\r\n","tokenizer = Tokenizer()\r\n","\r\n","data = \"In the town of Athy one Jeremy Lanigan \\n Battered away\"\r\n","corpus = data.lower().split('\\n') # convert to lowercase\r\n","\r\n","tokenizer.fit_on_texts(corpus) # encode (create token dictionary)\r\n","total_words = len(tokenizer.word_index) + 1 # total number of words\r\n","\r\n","print(corpus)\r\n","print(total_words)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["['in the town of athy one jeremy lanigan ', ' battered away']\n","11\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R6NkNOYk2A4G"},"source":["## Training the data"]},{"cell_type":"code","metadata":{"id":"Czqc4kAb2Crg"},"source":["input_sequences = []\r\n","for line in corpus:\r\n","  token_list = tokenizer.texts_to_sequences([line])[0]\r\n","  for i in range(1, len(token_list)):\r\n","    n_gram_sequence = token_list[:i+1]\r\n","    input_sequences.append(n_gram_sequence)"],"execution_count":null,"outputs":[]}]}