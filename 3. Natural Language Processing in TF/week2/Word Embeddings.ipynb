{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word Embeddings.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMCeMPgPO77kG3dgetqWJDG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"v9nwQReN6xSA"},"source":["## A conversation with Andrew Ng\r\n","**Word Embedding** : for representing the semantics of a word\r\n","> Tensorflow에서 word embedding 기능 제공\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"9tJOxjdN7w-4"},"source":["## Introduction\r\n","**Embedding** : 특정 단어들과 그것과 연관된 단어들은 다차원 공간에서 벡터들로 군집화된다는 개념에서 나온 것\r\n","> ex. 특정 단어들은 '긍정' 방향을 향하고, 특정 단어들은 '부정' 방향을 향함"]},{"cell_type":"markdown","metadata":{"id":"juKxGMz98fKA"},"source":["## The IMBD Dataset\r\n","텐서플로우의 데이터셋 라이브러리인 **TensorFlow Data Services (TFTS)**에는 다양한 데이터 셋들이 존재함 (image, audio, text 등)\r\n","> 그 중 text 데이터셋에 해당하는 **IMDB** (\"**imdb_reviews**\") : 긍정 or 부정으로 분류된 영화 리뷰 데이터들"]},{"cell_type":"markdown","metadata":{"id":"GNw1Afgh9qC1"},"source":["## Looking into the details"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IBXfdLaf9chd","executionInfo":{"status":"ok","timestamp":1612424130081,"user_tz":-540,"elapsed":2656,"user":{"displayName":"황지원","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3QkHCcEEvrl0G2GezbnmqYv8hfEf2EXH8W_8vNw=s64","userId":"04698757380061261060"}},"outputId":"6fcdefd4-de36-425a-fc7c-4b7de0aece08"},"source":["import tensorflow as tf\r\n","print(tf.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["2.4.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TOse-DzW942a"},"source":["# !pip install -q tensorflow-datasets => 구글 코랩에는 이미 설치되어 있음!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vWvfEkNoCSug"},"source":["#### [ cf. 2 methods of TensorFlow Datasets dataset loading ]\r\n","* 1. \r\n","```py \r\n","import tensorflow_datasets as tfds\r\n","tdds.load(\"imdb_reviews\")\r\n","```\r\n","* 2. \r\n","```py\r\n","import tensorflow as tf\r\n","tf.keras.datasets.imdb.load_data()\r\n","```"]},{"cell_type":"code","metadata":{"id":"FaoZ0DdTBmMS"},"source":["import tensorflow_datasets as tfds\r\n","\r\n","imdb, info = tfds.load(\"imdb_reviews\", with_info = True, as_supervised = True) # imda data, metadata 반환 \r\n","\r\n","\r\n","\r\n","import numpy as np\r\n","\r\n","train_data, test_data = imdb['train'], imdb['test'] # imdb : train과 test set으로 나누어져 있음! (각 25000 씩)\r\n","\r\n","training_sentences = []\r\n","training_labels = []\r\n","\r\n","testing_sentences = []\r\n","training_labels = []\r\n","\r\n","# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\r\n","for s, l in train_data:\r\n","  training_sentences.append(str(s.numpy()))\r\n","  training_labels.append(l.numpy())\r\n","\r\n","for s, l in test_data:\r\n","  testing_sentences.append(str(s.numpy()))\r\n","  testing_labels.append(l.numpy())\r\n","\r\n","# 1 : 긍정 label\r\n","# 0 : 부정 label\r\n","\r\n","training_labels_final = np.array(training_labels)\r\n","testing_labels_final = np.array(testing_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gjG4f-kPLCUb"},"source":["# tokenizing sentences\r\n","\r\n","# set hyperparameters (to make easier to change)\r\n","vocab_size = 10000\r\n","embedding_dim = 16\r\n","max_length = 120\r\n","trunc_type = 'post'\r\n","oov_tok = \"<OOV>\"\r\n","\r\n","from tensorflow.keras.preprocessing.text import Tokenizer\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","\r\n","tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok) \r\n","tokenizer.fit_on_texts(training_sentences)  \r\n","word_index = tokenizer.word_index  # training sentences로 만들어진 word_index\r\n","sequences = tokenizer.texts_to_sequences(training_sentences) \r\n","padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type) # padding and crop\r\n","\r\n","testing_sentences = tokenizer.texts_to_sequences(testing_sentences) \r\n","testing_padded = pad_sequences(testing_sequences, maxlen=max_length) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhEM_eUINA1H"},"source":["# define neural network ==> add Embedding!!\r\n","\r\n","model = tf.keras.Sequential([\r\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length), # 임베딩!! 텍스트 감정분석 핵심!!\r\n","    tf.keras.layers.Flatten(),\r\n","    tf.keras.layers.Dense(6, activation='relu'),\r\n","    tf.keras.layers.Dense(1, activation='sigmoid')                            \r\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"soyIICa9UBMf"},"source":["## How can we use vectors?\r\n","* 비슷한 의미의 단어들은 비슷한 벡터들로 할당됨 (함께 모임)\r\n","* 모델 학습 : 벡터와 그것의 label을 학습하는 것\r\n","> ==> **embedding**\r\n","* embedding 결과 : 2차원 배열 (문장의 길이, embedding 차원 수)\r\n","  \r\n","    \r\n","* Flatten() 대신 **GlobalAveragePooling1D()** 사용 :  벡터 요소들 전체의 평균들을 flatten\r\n","> 좀 더 느리게 학습되지만, 더 정확한 결과 도출\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"04k-GkG-UDwP"},"source":["model = tf.keras.Sequential([\r\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length), # 임베딩!\r\n","    tf.keras.layers.GlobalAveragePooling1D(),\r\n","    tf.keras.layers.Dense(6, activation='relu'),\r\n","    tf.keras.layers.Dense(1, activation='sigmoid')                            \r\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kqti5oCQEHIx"},"source":["## More into the details"]},{"cell_type":"code","metadata":{"id":"9jWLV7iFEJCi"},"source":["model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n","model.summary()\r\n","\r\n","\r\n","num_epochs = 10\r\n","model.fit(padded, # train_X\r\n","          training_labels_final, # train_Y\r\n","          epochs=num_epochs, # epochs\r\n","          validation_data=(testing_padded, testing_labels_final)) # val_set or callbacks"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g8ZBi8RtFo8J"},"source":["embedding 층의 학습 결과 확인 (weights)"]},{"cell_type":"code","metadata":{"id":"TU_8GJsdFn10"},"source":["e = model.layeres[0]\r\n","weights = e.get_weights()[0] # embedding 층 : 0번째\r\n","print(weights.shape) # shape : (vocab_size, embedding_dim) ==> (10000, 16)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7i9C4P0GBhc"},"source":["embedding 층의 weights.shape : (10000, 16)\r\n","> 즉, 말뭉치에 총 10000개의 단어들 존재, 16차원 배열로 embedding 작업 중!"]},{"cell_type":"code","metadata":{"id":"4i6ZWC3UGgWS"},"source":["# reverse word index (키-값 순서 바꾸기) ==> to be able to plot it\r\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ysJ3a3F9HF4m"},"source":["import io\r\n","\r\n","out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\r\n","out_m = io.open('meta.tsv', 'w', encoding='utf-8')\r\n","# 위에서 reverse한 dict 이용! (숫자 인덱스로 접근)\r\n","for word_num in range(1, vocab_size):\r\n","  word = reverse_word_index[word_num]\r\n","  embeddings = weights[word_num]\r\n","  out_m.write(word + \"\\n\") # 메타 데이터 파일\r\n","  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\") # 벡터 파일\r\n","out_v.close()\r\n","out_m.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"64ajJ-7xzyQv"},"source":[""],"execution_count":null,"outputs":[]}]}