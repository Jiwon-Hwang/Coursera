{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sequence models.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM8tdmtz6lyEUKF1CH++vOB"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"n8DureSabcjb"},"source":["## A conversation with Andrew Ng\r\n","단어들의 순서를 고려하기 위해서 RNN, GIO, LSTM과 같은 모델들을 사용함\r\n","> for. **자연어 처리**  "]},{"cell_type":"markdown","metadata":{"id":"to3NfSXWgFDo"},"source":["## Introduction\r\n","* f(data, labels) = Rules  \r\n","  * input : data, labels\r\n","  * output : rules  \r\n","\r\n","> 이러한 방식은 단어들의 순서를 고려하지 않은 것!\r\n","  \r\n","* f(x0) = y0 --> f(x1) = y1 ...  \r\n","\r\n","> 이러한 방식처럼 이전 결과가 다음 입력에도 영향끼침! (가중치 갱신)    \r\n","  \r\n","> **\"RNN\"** : Recurrent Neural Network!"]},{"cell_type":"markdown","metadata":{"id":"hbfW0iYvhyYG"},"source":["## LSTMs\r\n","* **LSTM : an update to RNN (Long Short-Term Memory)**\r\n","> LSTM : RNN의 일종\r\n","* Cell states : pipeline of contexts (which LSTMs have)\r\n","> * 방향, 양방향 모두 가능 (나중 문맥이 이전 문맥에 영향줄 수 있음)\r\n","> * **Bidirectional!** (<-> feed forward)"]},{"cell_type":"markdown","metadata":{"id":"kdY8TYL8jXGv"},"source":["## Implementing LSTMs in code"]},{"cell_type":"code","metadata":{"id":"XAPSCEIajZMq"},"source":["model = tf.keras.Sequential([\r\n","    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),\r\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), # 64 : the number of outputs of this layer\r\n","    tf.keras.layers.Dense(64, activation='relu'),\r\n","    tf.keras.layers.Dense(1, activation='sigmoid')                            \r\n","])\r\n","\r\n","# tf.keras.layers.LSTM(64) ==> 64 : the number of outputs of this layer\r\n","# 하지만 Output Shape에는 128! ==> 양방향이므로 2배로 늘림\r\n","# 이 LSTM층을 tf.keras.layers.Bidirectional()로 감싸면 cell state를 양방향으로 만듦"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"47N6RvH2pec9"},"source":["model = tf.keras.Sequential([\r\n","    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),\r\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)), # 다음 LSTM에 먹일때 인자 추가!\r\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), \r\n","    tf.keras.layers.Dense(64, activation='relu'),\r\n","    tf.keras.layers.Dense(1, activation='sigmoid')                            \r\n","])\r\n","\r\n","# return_sequences = True : 이 LSTM 층의 output과 다음 LSTM 층의 input을 맞춤(match)\r\n","# 즉, 메모리 셀이 모든 시점(time step)에 대해서 은닉 상태값(hidden state)을 출력 (중간 출력들을 모두 사용!)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9DvDHZ2r3A_"},"source":["## Accuracy and loss\r\n","1 layer LSTM과 2 layer LSTM의 accuracy 비교\r\n","* epoch에 따른 acc 변화 양상 곡선의 부드러움 정도가 다름! (두 층일 때 더 자연스럽게 변함)\r\n","* 10 epochs -> 50 epochs로 바꾸면 더 들쭉날쭉 정도가 크게 차이남\r\n","* 결과적으로 마지막엔 acc가 더 좋을지 몰라도, 모델의 전체적인 정확성은 떨어질 수 있음 (신뢰도)"]},{"cell_type":"markdown","metadata":{"id":"Arr-VHOCtPc2"},"source":["## Looking into the code\r\n","* without LSTM (with flattening, pooling) \r\n","  * 85% accuracy\r\n","  * loss와 val_loss 모두 점점 평평해짐\r\n","* with LSTM (with Bidirectional) \r\n","  * 97.5% accuracy\r\n","  * loss는 점점 평평해지며 줄어들지만, val_loss는 점점 늘어남..?!\r\n","  > **overfitting!**  \r\n","\r\n","  \r\n","*따라서 training parameter들을 조정하는 것이 중요하다!!*\r\n"]},{"cell_type":"code","metadata":{"id":"4TjjhnKcteGQ"},"source":["model = tf.keras.Sequential([\r\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\r\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\r\n","    tf.keras.layers.Dense(24, activation='relu'),\r\n","    tf.keras.layers.Dense(1, activation='sigmoid')   \r\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ud-LhO2Awqeb"},"source":["## Using a convolutional network\r\n","다양한 층으로 모델 구성하여 성능 비교해보기\r\n","1. flattening & pooling 층 이용\r\n","2. Bidirectional(LSTM) 층 이용\r\n","3. **convolution 층 이용** ==> 이번에 비교해 볼 것 \r\n","> 자연어 처리에서는 Conv2D가 아니라 **Conv1D** 이용"]},{"cell_type":"code","metadata":{"id":"rmXGfOHiwvdw"},"source":["model = tf.keras.Sequential([\r\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\r\n","    tf.keras.layers.Conv1D(128, 5, activation='relu'), # generate 128 filters, filter size : 5\r\n","    tf.keras.layers.GlobalMaxPooling1D(),\r\n","    tf.keras.layers.Dense(24, activation='relu'),\r\n","    tf.keras.layers.Dense(1, activation='sigmoid')   \r\n","])\r\n","\r\n","# 즉, 5 단어마다 128개의 필터들 존재! (한번 컨볼루션 5개 단어로 할때마다 128종류의 필터들 적용 => 여러 특징맵 생성)\r\n","# return : 1d array\r\n","# filter size가 5이므로, 컨볼루션 수행 시 문장 앞뒤로 단어 2개씩 잘라내게 됨! (문장 길이 : 120 --> 116)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUYqNBK25Kki"},"source":["## Going back to the IMDB dataset\r\n","1. **Flatten()**층 사용 시 : 171533개의 매개변수\r\n","  * 단, 1 epoch 당 5초\r\n","2. Bidirectional()층 (**LSTM**) 사용 시 : 30129개의 매개변수\r\n","  * 단, 1 epoch 당 43초\r\n","  * accuracy도 더 낫지만, overfitting 발생\r\n","3.  Bidirectional()층 (**GRU**) 사용 시 : 169997개의 매개변수\r\n","  * 단, 1 epoch 당 20초\r\n","  * accuracy도 좋고, 약간의 overfitting 존재\r\n","\r\n","4. **Conv1D()**층 사용 시 : 171149개의 매개변수 \r\n","* 단, 1 epoch 당 6초\r\n","* accuracy : 100%에 가깝고, overfitting 발생\r\n","\r\n","\r\n","> cf. GRU : different type of RNN"]},{"cell_type":"code","metadata":{"id":"rlNmPZTH5wMt"},"source":["import tensorflow_datasets as tfds\r\n","imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\r\n","\r\n","# Model Definition with LSTM\r\n","model = tf.keras.Sequential([\r\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\r\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\r\n","    tf.keras.layers.Dense(6, activation='relu'),\r\n","    tf.keras.layers.Dense(1, activation='sigmoid')   \r\n","])\r\n","\r\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n","\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bc2mVz5j8rG5"},"source":["## Tips from Laurance\r\n","* 각각 다른 층이나 다른 옵션들로 결과 비교해보기\r\n","* overfitting을 방지하는 방법 생각해보기"]}]}